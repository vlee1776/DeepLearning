{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home 4: Build a CNN for image recognition.\n",
    "\n",
    "### Name: Vincent Lee\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run the code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "    * Missing **the output after execution** will not be graded.\n",
    "    \n",
    "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo. (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/HM4/HM4.html\n",
    "\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
    "\n",
    "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
    "\n",
    "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
    "\n",
    "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
    "\n",
    "\n",
    "## Google Colab\n",
    "\n",
    "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
    "\n",
    "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
    "\n",
    "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
    "\n",
    "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    samples = len(y)\n",
    "    ret = numpy.zeros((samples,num_class))\n",
    "    for i in range(samples):\n",
    "        ret[i,y[i]] = 1\n",
    "    return ret\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 2, 2, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 7,905,706\n",
      "Trainable params: 7,899,434\n",
      "Non-trainable params: 6,272\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 1E-4 # to be tuned!\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vincent/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/10\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 1.5581 - acc: 0.4383 - val_loss: 1.2437 - val_acc: 0.5530\n",
      "Epoch 2/10\n",
      "1563/1562 [==============================] - 36s 23ms/step - loss: 1.1892 - acc: 0.5796 - val_loss: 1.0262 - val_acc: 0.6283\n",
      "Epoch 3/10\n",
      "1563/1562 [==============================] - 36s 23ms/step - loss: 1.0080 - acc: 0.6465 - val_loss: 1.0379 - val_acc: 0.6399\n",
      "Epoch 4/10\n",
      "1563/1562 [==============================] - 36s 23ms/step - loss: 0.8931 - acc: 0.6893 - val_loss: 0.8318 - val_acc: 0.7118\n",
      "Epoch 5/10\n",
      "1563/1562 [==============================] - 36s 23ms/step - loss: 0.7999 - acc: 0.7232 - val_loss: 0.8458 - val_acc: 0.7067\n",
      "Epoch 6/10\n",
      "1563/1562 [==============================] - 36s 23ms/step - loss: 0.7291 - acc: 0.7499 - val_loss: 0.7605 - val_acc: 0.7355\n",
      "Epoch 7/10\n",
      "1563/1562 [==============================] - 36s 23ms/step - loss: 0.6806 - acc: 0.7652 - val_loss: 0.7124 - val_acc: 0.7552\n",
      "Epoch 8/10\n",
      "1563/1562 [==============================] - 36s 23ms/step - loss: 0.6345 - acc: 0.7813 - val_loss: 0.6968 - val_acc: 0.7627\n",
      "Epoch 9/10\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 0.5875 - acc: 0.7959 - val_loss: 0.6020 - val_acc: 0.7924\n",
      "Epoch 10/10\n",
      "1563/1562 [==============================] - 36s 23ms/step - loss: 0.5597 - acc: 0.8057 - val_loss: 0.6870 - val_acc: 0.7640\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "### Data Augmentation\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "datagen.fit(x_tr)\n",
    "\n",
    "\n",
    "history = model.fit_generator(datagen.flow(x_tr, y_tr, batch_size=32), steps_per_epoch=len(x_train) / 32, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 1E-4 # to be tuned!\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 1.5452 - acc: 0.4401\n",
      "Epoch 2/10\n",
      "1563/1562 [==============================] - 32s 21ms/step - loss: 1.1794 - acc: 0.5790\n",
      "Epoch 3/10\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 1.0034 - acc: 0.6463\n",
      "Epoch 4/10\n",
      "1563/1562 [==============================] - 32s 21ms/step - loss: 0.8834 - acc: 0.6938\n",
      "Epoch 5/10\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 0.7993 - acc: 0.7234\n",
      "Epoch 6/10\n",
      "1563/1562 [==============================] - 32s 21ms/step - loss: 0.7403 - acc: 0.7436\n",
      "Epoch 7/10\n",
      "1563/1562 [==============================] - 32s 21ms/step - loss: 0.6887 - acc: 0.7606\n",
      "Epoch 8/10\n",
      "1563/1562 [==============================] - 32s 21ms/step - loss: 0.6437 - acc: 0.7783\n",
      "Epoch 9/10\n",
      "1563/1562 [==============================] - 32s 21ms/step - loss: 0.6126 - acc: 0.7895\n",
      "Epoch 10/10\n",
      "1563/1562 [==============================] - 32s 21ms/step - loss: 0.5746 - acc: 0.8025\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "### Data Augmentation\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "datagen.fit(x_tr)\n",
    "\n",
    "history = model.fit_generator(datagen.flow(x_train, y_train_vec, batch_size=32), \n",
    "                              steps_per_epoch=len(x_train) / 32, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 365us/step\n",
      "loss = 0.7503989241600036\n",
      "accuracy = 0.7597000002861023\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
